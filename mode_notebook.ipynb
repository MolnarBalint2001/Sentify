{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VuLzF1RGWErS"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Contants\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "padding_type = \"post\"\n",
        "truncating_type = \"post\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    # TensorFlow Datasets IMDb adathalmaz betöltése\n",
        "    dataset, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
        "    train_data, test_data = dataset[\"train\"], dataset[\"test\"]\n",
        "\n",
        "    train_texts, train_labels = [], []\n",
        "    for text, label in train_data:\n",
        "        train_texts.append(text.numpy().decode(\"utf-8\"))\n",
        "        train_labels.append(label.numpy())\n",
        "\n",
        "    test_texts, test_labels = [], []\n",
        "    for text, label in test_data:\n",
        "        test_texts.append(text.numpy().decode(\"utf-8\"))\n",
        "        test_labels.append(label.numpy())\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels\n"
      ],
      "metadata": {
        "id": "c9bSawlMOBmV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess dataset\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Szövegtisztítás (kisbetűsítés, speciális karakterek eltávolítása, stb.)\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, \"[^a-z ]\", \"\")\n",
        "    return text.numpy().decode(\"utf-8\")\n",
        "\n",
        "# 2. Adatok előfeldolgozása\n",
        "def preprocess_data(texts, labels):\n",
        "    # Szövegtisztítás alkalmazása\n",
        "    cleaned_texts = [clean_text(tf.constant(text)) for text in texts]\n",
        "\n",
        "    # Tokenizálás és szekvenciák pad-elése\n",
        "    tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(cleaned_texts)\n",
        "    sequences = tokenizer.texts_to_sequences(cleaned_texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=200, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    return padded_sequences, np.array(labels), tokenizer"
      ],
      "metadata": {
        "id": "Kz8Oabmyhh7m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentiment model\n",
        "\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=200),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "41GUCYZShxqp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "train_texts, train_labels, test_texts, test_labels = load_data()\n",
        "\n",
        "train_sequences, train_labels, tokenizer = preprocess_data(train_texts, train_labels)\n",
        "test_sequences, test_labels, _ = preprocess_data(test_texts, test_labels)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_sequences, train_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "no9UM-P7ic78"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=5,  # Nagyobb adathalmaz miatt csökkenthetjük az epoch-ok számát\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt3AplKcjrjD",
        "outputId": "abd1bec7-a5ef-413c-bacb-a76b3aac5368"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.6230 - loss: 0.6138 - val_accuracy: 0.8504 - val_loss: 0.3491\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 34ms/step - accuracy: 0.8799 - loss: 0.3240 - val_accuracy: 0.8648 - val_loss: 0.3331\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 33ms/step - accuracy: 0.9181 - loss: 0.2403 - val_accuracy: 0.8588 - val_loss: 0.3538\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9473 - loss: 0.1577 - val_accuracy: 0.8556 - val_loss: 0.4216\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9553 - loss: 0.1395 - val_accuracy: 0.8438 - val_loss: 0.4992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing and validation\n",
        "\n",
        "\n",
        "# 6. Tesztelés\n",
        "loss, accuracy = model.evaluate(test_sequences, test_labels)\n",
        "print(f\"Teszt pontosság: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Tokenizer mentése későbbi használatra\n",
        "def save_tokenizer(tokenizer, path=\"tokenizer.json\"):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(tokenizer.to_json())\n",
        "\n",
        "save_tokenizer(tokenizer)\n",
        "\n",
        "# Modell mentése\n",
        "model.save(\"sentiment_model_imdb.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrNDBTKEj0Iv",
        "outputId": "3f39e58e-b713-49fb-e3bc-fe22c67cc30d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 13ms/step - accuracy: 0.5216 - loss: 1.7619\n",
            "Teszt pontosság: 52.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom input\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "# Tokenizer betöltése\n",
        "def load_tokenizer(path=\"tokenizer.json\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        tokenizer_data = json.load(f)  # JSON objektum betöltése\n",
        "    tokenizer_json = json.dumps(tokenizer_data)  # Visszaalakítás stringgé\n",
        "    return tokenizer_from_json(tokenizer_json)\n",
        "\n",
        "# Egyedi szöveg tesztelése\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    def preprocess_single_text(text):\n",
        "        # Szöveg tisztítása\n",
        "        text = tf.constant(text)\n",
        "        text = tf.strings.lower(text)\n",
        "        text = tf.strings.regex_replace(text, \"[^a-z ]\", \"\")\n",
        "        return text.numpy().decode(\"utf-8\")\n",
        "\n",
        "    # Szöveg tisztítása és tokenizálása\n",
        "    cleaned_text = preprocess_single_text(text)\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=200, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    # Előrejelzés\n",
        "    prediction = model.predict(padded_sequence)[0][0]\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    return sentiment, prediction\n",
        "\n",
        "# Modell és tokenizer betöltése\n",
        "loaded_model = tf.keras.models.load_model(\"sentiment_model_imdb.h5\")\n",
        "loaded_tokenizer = load_tokenizer()\n",
        "\n",
        "# Szöveg tesztelése\n",
        "text_input = \"The movie was absolutely terrible. I had just wasted my time.\"\n",
        "sentiment, confidence = predict_sentiment(text_input, loaded_model, loaded_tokenizer)\n",
        "\n",
        "print(f\"Input: {text_input}\")\n",
        "print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raDDMFZImElQ",
        "outputId": "a20cecf8-7128-45a6-bc94-181323be5f15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step\n",
            "Input: The movie was absolutely terrible. I had just wasted my time.\n",
            "Predicted Sentiment: Negative (Confidence: 0.04)\n"
          ]
        }
      ]
    }
  ]
}